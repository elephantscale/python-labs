{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spacy_Exercise5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNZVen4nVqeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.lang.en import English\n",
        "\n",
        "with open(\"exercises/iphone.json\") as f:\n",
        "    TEXTS = json.loads(f.read())\n",
        "\n",
        "nlp = English()\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Two tokens whose lowercase forms match 'iphone' and 'x'\n",
        "pattern1 = [{____: ____}, {____: ____}]\n",
        "\n",
        "# Token whose lowercase form matches 'iphone' and an optional digit\n",
        "pattern2 = [{____: ____}, {____: ____, ___: ____}]\n",
        "\n",
        "# Add patterns to the matcher\n",
        "matcher.add(\"GADGET\", None, pattern1, pattern2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chuLbaXhWFqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.lang.en import English\n",
        "\n",
        "with open(\"exercises/iphone.json\") as f:\n",
        "    TEXTS = json.loads(f.read())\n",
        "\n",
        "nlp = English()\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
        "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True, \"OP\": \"?\"}]\n",
        "matcher.add(\"GADGET\", None, pattern1, pattern2)\n",
        "\n",
        "TRAINING_DATA = []\n",
        "\n",
        "# Create a Doc object for each text in TEXTS\n",
        "for ____ in ____:\n",
        "    # Match on the doc and create a list of matched spans\n",
        "    spans = [____[____:____] for match_id, start, end in matcher(doc)]\n",
        "    # Get (start character, end character, label) tuples of matches\n",
        "    entities = [(span.start_char, span.end_char, \"GADGET\") for span in spans]\n",
        "    # Format the matches as a (doc.text, entities) tuple\n",
        "    training_example = (____, {\"entities\": ____})\n",
        "    # Append the example to the training data\n",
        "    ____.____(____)\n",
        "\n",
        "print(*TRAINING_DATA, sep=\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9k7Us4HWOCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy\n",
        "\n",
        "# Create a blank 'en' model\n",
        "nlp = ____\n",
        "\n",
        "# Create a new entity recognizer and add it to the pipeline\n",
        "ner = ____\n",
        "____\n",
        "\n",
        "# Add the label 'GADGET' to the entity recognizer\n",
        "____.____"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-TeNRMsWf6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import random\n",
        "import json\n",
        "\n",
        "with open(\"exercises/gadgets.json\") as f:\n",
        "    TRAINING_DATA = json.loads(f.read())\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "ner = nlp.create_pipe(\"ner\")\n",
        "nlp.add_pipe(ner)\n",
        "ner.add_label(\"GADGET\")\n",
        "\n",
        "# Start the training\n",
        "____.____\n",
        "\n",
        "# Loop for 10 iterations\n",
        "for itn in range(____):\n",
        "    # Shuffle the training data\n",
        "    random.shuffle(TRAINING_DATA)\n",
        "    losses = {}\n",
        "\n",
        "    # Batch the examples and iterate over them\n",
        "    for batch in ____.____.____(TRAINING_DATA, size=2):\n",
        "        texts = [____ for text, entities in batch]\n",
        "        annotations = [____ for text, entities in batch]\n",
        "\n",
        "        # Update the model\n",
        "        ____.____(texts, annotations, losses=losses)\n",
        "        print(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d97hcPWxWlLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAINING_DATA = [\n",
        "    (\n",
        "        \"i went to amsterdem last year and the canals were beautiful\",\n",
        "        {\"entities\": [(10, 19, \"TOURIST_DESTINATION\")]},\n",
        "    ),\n",
        "    (\n",
        "        \"You should visit Paris once in your life, but the Eiffel Tower is kinda boring\",\n",
        "        {\"entities\": [(17, 22, \"TOURIST_DESTINATION\")]},\n",
        "    ),\n",
        "    (\"There's also a Paris in Arkansas, lol\", {\"entities\": []}),\n",
        "    (\n",
        "        \"Berlin is perfect for summer holiday: lots of parks, great nightlife, cheap beer!\",\n",
        "        {\"entities\": [(0, 6, \"TOURIST_DESTINATION\")]},\n",
        "    ),\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyZb3D2OWyQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAINING_DATA = [\n",
        "    (\n",
        "        \"Reddit partners with Patreon to help creators build communities\",\n",
        "        {\"entities\": [(____, ____, \"WEBSITE\"), (____, ____, \"WEBSITE\")]},\n",
        "    ),\n",
        "    (\"PewDiePie smashes YouTube record\", {\"entities\": [(____, ____, \"WEBSITE\")]}),\n",
        "    (\n",
        "        \"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\",\n",
        "        {\"entities\": [(____, ___, \"WEBSITE\")]},\n",
        "    ),\n",
        "    # And so on...\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNSaD38xW-RW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAINING_DATA = [\n",
        "    (\n",
        "        \"Reddit partners with Patreon to help creators build communities\",\n",
        "        {\"entities\": [(0, 6, \"WEBSITE\"), (21, 28, \"WEBSITE\")]},\n",
        "    ),\n",
        "    (\"PewDiePie smashes YouTube record\", {\"entities\": [____, (18, 25, \"WEBSITE\")]}),\n",
        "    (\n",
        "        \"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\",\n",
        "        {\"entities\": [(0, 6, \"WEBSITE\"), ____]},\n",
        "    ),\n",
        "    # And so on...\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}